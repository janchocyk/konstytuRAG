from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_openai import ChatOpenAI
from langchain_pinecone import PineconeVectorStore
from langchain.chains import create_history_aware_retriever
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.docstore.document import Document
import tiktoken


contextualize_q_system_prompt = """Biorąc pod uwagę historię czatu i ostatnie pytanie użytkownika \
które może odnosić się do informacji z historii czatu, sformułuj samodzielne pytanie \
które będzie uwzględniało kontekst historii czatu. NIE odpowiadaj na pytanie, \
tylko przeformułuj je w razie potrzeby, a w przeciwnym razie zwróć je bez zmian.
"""

qa_system_prompt = """Jesteś asystentem, który odpowiada na pytania dotyczące Konstytucji Rzeczpospolitej Polskiej. \
Odpowiedz najuczciwiej i najdokładniej jak tylko umiesz, używając tylko i wyłącznie informacj zawartych w przekazanym kontekście. \
Jeżeli nie znajdziesz odpowiedzi na pytanie w kontekście odpowiedz: 'Niestety nie znam odpowiedzi'

{context}"""


class konstytuRAG:
    """
    Description: The class is responsible for initializing the components from which the RAG will be built later on. 
    Remember that for it to work correctly, the environment variables PINECONE_API_KEY and OPENAI_API_KEY must be set, 
    and a Pinecone vector index must be created (details in README.md).

    Attributes:
    - embeddings (langchain_community.embeddings.huggingface.HuggingFaceEmbeddings)
    - llm (langchain_openai.chat_models.base.ChatOpenAI)
    - vectorstore (langchain_pinecone.vectorstores.PineconeVectorStore)
    - rag_chain (None): after init_rag_chain -> (langchain_core.runnables.base.RunnableBinding)

    Methods:
    - init_rag_chain: returns the RAG chain
    - get_answer: takes parameters from the user, then calls self.rag_chain.invoke to generate a response
    """
    def __init__(self) -> None:
        self.embeddings = SentenceTransformerEmbeddings(model_name='sdadas/mmlw-roberta-large')
        self.llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)
        self.vectorstore = PineconeVectorStore.from_existing_index(index_name='konstytucja', embedding=self.embeddings)
        self.rag_chain = None
    
    def init_rag_chain(self):
        '''
        Init RAG chain. Set a prompt.
        '''
        retriever = self.vectorstore.as_retriever(search_kwargs={"k": 2})

        contextualize_q_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", contextualize_q_system_prompt),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}"),
            ]
        )
        history_aware_retriever = create_history_aware_retriever(
            self.llm, retriever, contextualize_q_prompt
        )

        qa_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", qa_system_prompt),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}"),
            ]
        )
        question_answer_chain = create_stuff_documents_chain(self.llm, qa_prompt)

        self.rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)


    def get_answer(self, question: str, chat_history: list) -> tuple:
        """
        Description: A method that handles generating a response from the RAG and its post-processing.

        Parameters:
        - question (str): The user's question.
        - chat_history (list): A list of historical messages from the conversation.

        Returns:
        - ai_answer['answer'], source (tuple): The response generated by RAG (str) and source information (str), in the form of a tuple.
        """
        ai_answer = self.rag_chain.invoke({'input': question, 'chat_history': chat_history})
        if ai_answer['answer'] == 'Niestety nie znam odpowiedzi.':
            source = 'Źródła: ----------' 
        else:
            source = 'Źródła: ' + ai_answer['context'][0].metadata['source'] + ', ' + ai_answer['context'][1].metadata['source']

        return ai_answer['answer'], source


def num_tokens_from_string(string: str, model_name: str) -> int:
    '''
    Description: Function used for counting tokens for OpenAI models.
    '''
    encoding = tiktoken.encoding_for_model(model_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens
